#conf.py will parse the yaml and extract parameters based on what is specified

#will do stuff in fs_path / [username] / signal_data | shot_lists | processed shots, etc.

fs_path: '/tigress'
target: 'maxhinge' #'binary' #'hinge' 
num_gpus: 4

paths:
    signal_prepath: '/signal_data/' #/signal_data/jet/
    shot_list_dir: '/shot_lists/'
    tensorboard_save_path: '/Graph/'
    data: 'jet_data'
    specific_signals: [] #['q95','li','ip','betan','energy','lm','pradcore','pradedge','pradtot','pin','torquein','tmamp1','tmamp2','tmfreq1','tmfreq2','pechin','energydt','ipdirect','etemp_profile','edens_profile'] #if left empty will use all valid signals defined on a machine. Only use if need a custom set
    executable: "mpi_learn.py"
    shallow_executable: "learn.py"

data:
    signal_to_augment: 'plasma current' #or None
    augmentation_mode: 'noise'
    augment_during_training: True
    cut_shot_ends: True
    T_min_warn: 30
    recompute: False
    recompute_normalization: False
    #specifies which of the signals in the signals_dirs order contains the plasma current info
    current_index: 0
    plotting: False
    #train/validate split
    #how many shots to use
    use_shots: 200000 #1000 #200000
    positive_example_penalty: 1.0 #by what factor to upweight positive examples?
    #normalization timescale
    dt: 0.001
    #maximum TTD considered
    T_max: 1000.0
    #The shortest works best so far: less overfitting. log TTd prediction also works well. 0.5 better than 0.2
    T_warning: 1.024 #1.024 #0.512 #0.25 #1.0 #1.0 #warning time in seconds
    current_thresh: 750000
    current_end_thresh: 10000
    #the characteristic decay length of the decaying moving average window
    window_decay: 2
    #the width of the actual window
    window_size: 10
    #TODO optimize
    normalizer: 'var'
    equalize_classes: False
    # shallow_sample_prob: 0.01 #the fraction of samples with which to train the shallow model
    shallow_num_samples: 10000 #the number of samples to use for training
    floatx: 'float64'

model:
    shallow: False
    shallow_model: 
        num_samples: 10000
        type: "svm" #"random_forest"
        n_estimators: 50 #for random forest
        max_depth: 20 #for random forest
        C: 1.0 #for svm
        kernel: "rbf" #rbf, sigmoid, linear, poly, for svm
        skip_train: False #should a finished model be loaded if available
    #length of LSTM memory
    pred_length: 200
    pred_batch_size: 128
    #TODO optimize
    length: 128
    skip: 1
    #hidden layer size
    #TODO optimize  
    rnn_size: 200
    #size 100 slight overfitting, size 20 no overfitting. 200 is not better than 100. Prediction much better with size 100, size 20 cannot capture the data.
    rnn_type: 'LSTM'
    #TODO optimize
    rnn_layers: 2
    num_conv_filters: 10
    size_conv_filters: 3
    num_conv_layers: 2
    pool_size: 2
    dense_size: 200
    #have not found a difference yet
    optimizer: 'adam'
    clipnorm: 10.0
    regularization: 0.0
    dense_regularization: 0.01
    #1e-4 is too high, 5e-7 is too low. 5e-5 seems best at 256 batch size, full dataset and ~10 epochs, and lr decay of 0.90. 1e-4 also works well if we decay a lot (i.e ~0.7 or more)
    lr: 0.00001 #0.0005 #for adam plots 0.0000001 #0.00005 #0.00005 #0.00005
    lr_decay: 0.9 #0.98 #0.9
    stateful: True
    return_sequences: True
    dropout_prob: 0.3
    #only relevant if we want to do mpi training. The number of steps with a single replica
    warmup_steps: 0
    ignore_timesteps: 100 #how many initial timesteps to ignore during evaluation (to let the internal state settle)
    backend: 'tensorflow'
training:
    as_array_of_shots: True
    shuffle_training: True
    train_frac: 0.75
    validation_frac: 0.33
    batch_size: 256
    #THIS WAS THE CULPRIT FOR NO TRAINING! Lower than 1000 performs very poorly
    max_patch_length: 100000
    #How many shots are we loading at once?
    num_shots_at_once: 200
    num_epochs: 400
    use_mock_data: False
    data_parallel: False
    hyperparam_tuning: False
    batch_generator_warmup_steps: 0
    num_batches_minimum: 200 #minimum number of batches per epoch
    ranking_difficulty_fac: 1.0 #how much to upweight incorrectly classified shots during training
callbacks:
    list: ['earlystop']
    metrics: ['val_loss','val_roc','train_loss']
    mode: 'max'
    monitor: 'val_roc'
    patience: 3
    write_grads: False
