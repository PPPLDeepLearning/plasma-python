#conf.py will parse the yaml and extract parameters based on what is specified

#will do stuff in fs_path / [username] / signal_data | shot_lists | processed shots, etc.

# for example tigress/alexeys
fs_path: '/tigress' #'/cscratch/share/frnn' #'/tigress'
target: 'maxhinge' #'binary' #'ttd' #'hinge'

paths:
    shot_files: ['CWall_clear.txt','CFC_unint.txt']
    shot_files_test: ['BeWall_clear.txt','ILW_unint.txt']
    signals_dirs: [['jpf/da/c2-ipla'], # Plasma Current [A]
                ['jpf/da/c2-loca'], # Mode Lock Amplitude [A]
                ['jpf/db/b5r-ptot>out'], #Radiated Power [W]
                ['jpf/gs/bl-li<s'], #Plasma Internal Inductance
                ['jpf/gs/bl-fdwdt<s'], #Stored Diamagnetic Energy (time derivative) [W] Might cause a lot of false positives!
                ['jpf/gs/bl-ptot<s'], #total input power [W]
                ['jpf/gs/bl-wmhd<s'], #total stored diamagnetic energy
                #density signals [m^-2]
                #4 vertical channels and 4 horizontal channels
                ['jpf/df/g1r-lid:002',
                 'jpf/df/g1r-lid:003',
                 'jpf/df/g1r-lid:004',
                 'jpf/df/g1r-lid:005',
                 'jpf/df/g1r-lid:006',
                 'jpf/df/g1r-lid:007',
                 'jpf/df/g1r-lid:008']]
    signal_prepath: '/signal_data/' #/signal_data/jet/
    shot_list_dir: '/shot_lists/'
    signals_masks: [['jpf/df/g1r-lid:003',
                 'jpf/df/g1r-lid:004',
                 'jpf/df/g1r-lid:005',
                 'jpf/df/g1r-lid:006',
                 'jpf/df/g1r-lid:007',
                 'jpf/df/g1r-lid:008']]
    positivity_mask: [['jpf/da/c2-ipla'], 
                      ['jpf/gs/bl-fdwdt<s']]
    tensorboard_save_path: '/Graph/'

data:
    cut_shot_ends: True
    T_min_warn: 30
    recompute: False
    recompute_normalization: False
    #specifies which of the signals in the signals_dirs order contains the plasma current info
    current_index: 0
    plotting: False
    #train/validate split
    #how many shots to use
    use_shots: 200000 #1000 #200000
    #normalization timescale
    dt: 0.001
    #maximum TTD considered
    T_max: 1000.0
    #The shortest works best so far: less overfitting. log TTd prediction also works well. 0.5 better than 0.2
    T_warning: 1.0 #0.25 #1.0 #1.0 #warning time in seconds
    current_thresh: 750000
    current_end_thresh: 10000
    #the characteristic decay length of the decaying moving average window
    window_decay: 2
    #the width of the actual window
    window_size: 10
    #TODO optimize
    normalizer: 'var'

model:
    #length of LSTM memory
    pred_length: 200
    pred_batch_size: 128
    #TODO optimize
    length: 128
    skip: 1
    #hidden layer size
    #TODO optimize  
    rnn_size: 200
    #size 100 slight overfitting, size 20 no overfitting. 200 is not better than 100. Prediction much better with size 100, size 20 cannot capture the data.
    rnn_type: 'LSTM'
    #TODO optimize
    rnn_layers: 2
    num_conv_filters: 10
    size_conv_filters: 3
    num_conv_layers: 2
    pool_size: 2
    dense_size: 16
    #have not found a difference yet
    optimizer: 'adam'
    clipnorm: 10.0
    regularization: 0.0
    #1e-4 is too high, 5e-7 is too low. 5e-5 seems best at 256 batch size, full dataset and ~10 epochs, and lr decay of 0.90. 1e-4 also works well if we decay a lot (i.e ~0.7 or more)
    lr: 0.0001 #0.00005 #0.00005
    lr_decay: 0.9 #0.9
    stateful: True
    return_sequences: True
    dropout_prob: 0.3
    #only relevant if we want to do mpi training. The number of steps with a single replica
    warmup_steps: 0
    ignore_timesteps: 100 #how many initial timesteps to ignore during evaluation (to let the internal state settle)
    backend: 'tensorflow' #'theano' #'tensorflow' #theano


training:
    as_array_of_shots: True
    shuffle_training: True
    train_frac: 0.75
    validation_frac: 0.33
    batch_size: 256
    #THIS WAS THE CULPRIT FOR NO TRAINING! Lower than 1000 performs very poorly
    max_patch_length: 100000
    #How many shots are we loading at once?
    num_shots_at_once: 200

    num_epochs: 10
    use_mock_data: False
    data_parallel: False
callbacks:
    list: ['earlystop']
    metrics: ['val_loss','val_roc','train_loss']
    mode: 'auto'
    monitor: 'val_loss'
    patience: 2
    write_grads: False

plots:
    #LaTeX strings for performance analysis, sorted in lists by signal_group
    group_labels: [[' $I_{plasma}$ [A]'],
              [' Mode L. A. [A]'],
              [' $P_{radiated}$ [W]'],
              [' $P_{radiated}$ [W]'],
              [' $\rho_{plasma}$ [m^-2]'],
              [' $L_{plasma,internal}$'],
              ['$\frac{d}{dt} E_{D}$ [W]'],
              [' $P_{input}$ [W]'],
              ['$E_{D}$'],
                #ppf signal labels
              ['ECE unit?']]

    plot_masks: [['jpf/da/c2-loca'], # Mode Lock Amplitude [A]
                ['jpf/db/b5r-ptot>out'], #Radiated Power [W]
                ['jpf/gs/bl-li<s'], #Plasma Internal Inductance
                ['jpf/gs/bl-fdwdt<s'], #Stored Diamagnetic Energy (time derivative) [W] Might cause a lot of false positives!
                ['jpf/gs/bl-ptot<s'], #total input power [W]
                ['jpf/df/g1r-lid:002',
                 'jpf/df/g1r-lid:003',
                 'jpf/df/g1r-lid:004',
                 'jpf/df/g1r-lid:005',
                 'jpf/df/g1r-lid:006',
                 'jpf/df/g1r-lid:007',
                 'jpf/df/g1r-lid:008']]
